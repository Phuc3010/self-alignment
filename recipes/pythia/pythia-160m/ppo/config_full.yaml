use_seq2seq: False
trust_remote_code: True
use_peft: False
lora_alpha: 16
lora_r: 16

exp_name: ppo-pythia-160m
seed: 0
log_with: wandb
model_name: Dahoas/pythia-125M-static-sft
reward_model: text-classification:data/pythia-160m-rm-hh
ppo_epochs: 4
learning_rate: 1.0e-5
batch_size: 128
mini_batch_size: 8
gradient_accumulation_steps: 8
vf_coef: 0.5
cliprange: 2
use_score_norm: True
use_score_scaling: True
optimize_device_cache: True 
query_dataset: Anthropic/hh-rlhf



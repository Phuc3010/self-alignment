seed: 1
exp_name: kto_pythia410m
datasets:
- synthetic
mode: train
debug: false
use_fsdp: true
fsdp_port: 36839
wandb:
  enabled: true
  entity: null
  project: archangel
cache_dir: /data/models
local_run_dir: data/models/pythia1b_kto
do_first_eval: true
minimum_log_interval_secs: 1.0
intermediate_checkpoints: false
trainer: BasicTrainer
lr: 5.0e-07
n_epochs: 1
n_examples: null
optimizer: RMSprop
warmup_steps: 100
eval_every: 300
n_samples: 128
samples_dir: samples/
n_eval_examples: 512
saved_policy: data/models/pythia1b_kto/LATEST/policy.pt
top_p: 0.95
human_prefix: '<|user|>

  '
assistant_prefix: '<|assistant|>

  '
human_suffix: ''
assistant_suffix: ''
frac_unique_desirable: 1.0
frac_unique_undesirable: 1.0
model:
  name_or_path: EleutherAI/pythia-410m-deduped-v0
  tokenizer_name_or_path: null
  load_from: null
  block_name: GPTNeoXLayer
  policy_dtype: bfloat16
  fsdp_policy_mp: null
  reference_dtype: bfloat16
  max_grad_norm: 10.0
  v_head_max_grad_norm: 0.1
  max_length: 1024
  max_prompt_length: 512
  activation_checkpointing: true
  batch_size: 8
  gradient_accumulation_steps: 4
  eval_batch_size: 8
  use_flash_attention: true
loss:
  name: kto
  beta: 0.1
  trainer: KTOTrainer
  dataloader: UnpairedPreferenceDataLoader
  use_reference_model: true
  desirable_weight: 1.0
  undesirable_weight: 0.33
